# Internal Review (Round 1)

## Editor Summary

This paper evaluates the effect of India's electronic National Agriculture Market (e-NAM) platform on wholesale commodity prices using a staggered difference-in-differences design. Drawing on mandi-level price data from the CEDA Ashoka AgMarkNet dataset (2007-2025) for four commodities (onion, tomato, wheat, soybean), the authors exploit the phased rollout of e-NAM across 18 states between 2016 and 2018. The headline finding is a "storability divide": Callaway-Sant'Anna estimates show that e-NAM raised prices for storable commodities (wheat by 4.7%, soybean by 8.2%), while effects on perishable commodities (onion, tomato) cannot be credibly identified due to pre-trend violations. The paper is well-written with a compelling narrative, and the topic---digital market infrastructure in developing-country agriculture---is important and timely.

However, the paper suffers from a fundamental identification problem that undermines the reliability of its preferred CS-DiD estimates: the analysis sample contains essentially zero never-treated units for the key commodities (wheat, soybean), and the tight clustering of treatment cohorts between April 2016 and March 2018 means that "not-yet-treated" controls evaporate rapidly after treatment onset. The disagreement between the three heterogeneity-robust estimators (CS-DiD shows large positive effects, Sun-Abraham shows near-zero or negative effects, TWFE shows negative effects) is inadequately reconciled and suggests that the positive findings may be an artifact of the specific comparison group composition rather than a robust causal effect. The paper also has data construction issues---sampling only 6 districts per state rather than using the full universe of mandis---that limit external validity and raise concerns about selective coverage. In its current form, the paper requires a major revision to address the identification challenges before it can be considered for publication.

## Verdict: Major Revision

## Major Concerns

1. **The CS-DiD vs. Sun-Abraham disagreement is not resolved and is potentially fatal to the main claims.** The paper's headline results rest entirely on the Callaway-Sant'Anna estimator, which shows wheat at +4.7% and soybean at +8.2%. Yet the Sun-Abraham estimator---which is also heterogeneity-robust and designed to address the same Goodman-Bacon bias---shows wheat at -2.1% and soybean at -0.3% (Table 4). These are not minor quantitative differences; they are qualitative sign reversals for wheat and order-of-magnitude disagreements for soybean. The paper dismisses this by saying Sun-Abraham "reweights within a regression framework, making it more sensitive to the specific cohort structure in our unbalanced panel," but this is hand-waving. If two heterogeneity-robust estimators give opposite answers, the identification is fragile. A serious engagement would require: (a) a Goodman-Bacon decomposition showing the weight on each 2x2 comparison, (b) an explicit accounting of which comparison groups drive the CS-DiD result, (c) implementation of Borusyak, Jaravel, and Spiess (2024)---which is cited in the bibliography but never used---as a third heterogeneity-robust estimator, and (d) honest discussion of whether the positive CS-DiD result is driven by a small number of comparison cells with unusual price dynamics.

2. **The "not-yet-treated" control group is vanishingly thin and its adequacy is never verified.** Table 1 shows zero never-treated mandis for wheat and soybean in the regression sample. The paper relies entirely on "not-yet-treated" controls for CS-DiD, but never documents how many not-yet-treated mandis are available at each event-study horizon. The four treatment cohorts are clustered within a 24-month window (April 2016 to March 2018). For the Phase 1A cohort (treated April 2016), the only "not-yet-treated" controls are Phase 1B mandis (available for ~7 months), Phase 1C (available for ~11 months), and Phase 1D (available for ~24 months). Beyond March 2018, there are literally zero control units for any cohort. This means: (a) the long-horizon event-study coefficients that the paper interprets as "gradually emerging effects by the third year" may be entirely unidentified or identified from a single remaining cohort, (b) the overall ATT from CS-DiD is a weighted average where post-2018 treatment effects receive substantial weight but are identified from essentially no comparison, and (c) the GPT-5.2 advisor review (which was the one FAIL verdict) raised exactly this concern. The authors must present a table showing, for each event-study horizon, the number of treated units and the number of not-yet-treated control units available.

3. **The district sampling strategy compromises the sample and is not adequately justified.** The data fetch script (`01_fetch_data.R`, line 83-87) samples only 6 districts per state from the CEDA API, rather than fetching the full universe. This is a tractability choice that has profound consequences: the paper claims to cover "over 2,700 mandis" in the CEDA database but the regression sample uses only 370 mandis. More critically, the 6-district-per-state sampling is random (`set.seed(42)`) and may exclude major agricultural mandis that are central to e-NAM's operation. The paper never discusses this sampling, never reports what fraction of e-NAM mandis are captured, and never assesses whether the sampled mandis are representative. If the 6 sampled districts in Madhya Pradesh happen to exclude Neemuch (India's largest mandi for soybean), the soybean results could be entirely unrepresentative. This sampling must be explicitly disclosed in the paper, and robustness to alternative district draws should be assessed.

4. **The treatment is assigned at the wrong level and the paper understates how damaging this is.** Treatment is assigned at the state-phase level, meaning all mandis in a state receive treatment simultaneously. The paper acknowledges this introduces measurement error but claims it causes "attenuation bias" that makes estimates conservative. This is incorrect in a staggered DiD context. Misassigned treatment timing can create spurious pre-trends or post-trends because mandis assigned an incorrect treatment date will contribute anomalous group-time ATTs. More fundamentally, state-level treatment assignment means there is no within-state variation: every mandi in Gujarat is treated in April 2016, every mandi in Karnataka is treated in March 2018. The "staggered" variation is entirely between-state, which means the identifying variation comes from comparing states that adopted at different times. With only 4 treatment cohorts across 18 states, this is effectively a study with 4 treatment groups and 2 control states (Bihar, Assam)---far less statistical power than the 370-mandi sample size suggests. The effective number of clusters for inference is at most 20 states, and the state-clustered standard errors (which the paper reports only for TWFE, not for CS-DiD) show that wheat becomes insignificant (p = 0.258).

5. **The arrival quantity outcome---the third prediction of the conceptual framework---is never estimated.** Section 3.3 predicts that e-NAM should increase arrival quantities, and the research plan lists log arrivals as a primary outcome variable. Yet the paper never estimates arrival effects. The CEDA API provides arrival data (the fetch script requests prices, not quantities, despite the API supporting both). Omitting this outcome is a significant gap: if e-NAM raised prices without increasing arrivals, the mechanism is more ambiguous (it could reflect cost pass-through or seasonal effects rather than improved competition). If arrivals increased, it would strongly support the market integration story. The absence of this analysis, which was explicitly planned, weakens the paper's contribution.

6. **The price dispersion result---the primary prediction of the conceptual framework---is null, and this is under-discussed.** The conceptual framework in Section 3.1 predicts that e-NAM should reduce price dispersion (CV) across mandis. The estimated effect is -0.014 (p = 0.341)---a null result. Yet the paper treats this as a minor finding ("directionally consistent with convergence but not statistically significant") and moves on. For a paper about "price discovery" and "market integration," a null result on the primary measure of market integration deserves serious engagement. One natural interpretation is that e-NAM did not actually improve market integration---the price-level effects for wheat and soybean could reflect other confounders. Another is that the state-level CV measure is too coarse because treatment is also at the state level (all mandis in a state are treated simultaneously, so within-state CV may not capture the relevant between-state convergence). The paper should present between-state price dispersion measures and engage seriously with the tension between positive price-level effects and null dispersion effects.

## Minor Concerns

1. **The placebo test for perishable commodities is used both to invalidate those results AND to validate the design for storable commodities, which is circular.** The paper argues that onion and tomato fail the placebo test (significant pre-trends), proving the identification is invalid for those crops. But it then argues that wheat and soybean pass the placebo test, proving the identification is valid for those crops. This logic is only valid if the source of pre-trend violations for perishables is unrelated to storables---which is never established. If the pre-trend violations reflect systematic differences between early-adopting and late-adopting states (which is plausible given state-level treatment assignment), these differences could also affect storable commodities in ways that happen to be small enough to not reach statistical significance in a placebo test, especially with a small effective sample of states.

2. **The Bergquist and Dinerstein (2019/2020) citation has inconsistent dates.** In the text it is cited as "Bergquist (2019)" but the bibliography entry has `year={2020}` and lists the publication year as 2020 (AER Vol. 110, No. 12). This should be corrected.

3. **Several bibliography entries have incorrect or inconsistent formatting.** Acharya (2012) and Bardhan (1984) list "Oxford University Press" and "Columbia University Press" in the journal field rather than as publisher. Stiglitz (2002) lists "Oxford University Press" as the journal. Government of India (2006) is attributed to "deboer1996agricultural" as a BibTeX key. These are sloppy and would be caught by any copy editor, but they undermine confidence in attention to detail.

4. **The "Within R-squared" values in Table 2 are extremely low (0.0001 to 0.007), suggesting the treatment explains almost none of the within-mandi price variation.** While this is common in panel data settings with rich fixed effects, it should be discussed. A within R-squared of 0.0001 for tomato means the e-NAM indicator explains 0.01% of within-mandi price variation. This raises questions about the economic significance of the treatment even where it is statistically significant.

5. **The paper's title promises evidence about "price discovery" but the analysis only examines price levels and within-state price dispersion.** True price discovery effects would involve measures like the speed of spatial price transmission, the convergence of prices to the law of one price, or changes in the persistence of price deviations. The coefficient of variation is a static measure that does not capture the dynamic properties of price discovery. The title oversells the analysis.

6. **The abstract exceeds the stated 150-word limit.** The CLAUDE.md instructions specify that the abstract should be at most 150 words on page 1. The current abstract is approximately 175 words. While this is a soft guideline, it should be tightened.

7. **The Sun-Abraham estimates for onion (SE = 1.554) and tomato (SE = 31.037) in the robustness CSV are wildly imprecise, yet this is never discussed.** The robustness comparison CSV shows Sun-Abraham standard errors of 1.55 for onion and 31.04 for tomato. These enormous standard errors suggest severe collinearity or identification problems for these commodities under the Sun-Abraham framework. The paper reports em-dashes for these in Table 4, which is appropriate, but should briefly explain why these estimates explode.

8. **No power analysis or minimum detectable effect (MDE) calculation is provided.** The research plan promised high statistical power ("Even modest effects of 1-2% price changes should be detectable"), but the paper never reports an MDE. Given that the effective number of treated clusters is only 18 states across 4 cohorts, a formal power analysis would likely reveal that the study is substantially less powered than the raw mandi count suggests.

9. **The paper does not implement HonestDiD (Rambachan and Roth, 2023), which was listed as a planned robustness check in the research plan.** This sensitivity analysis would directly address the question of how robust the positive CS-DiD results are to violations of parallel trends---precisely the concern raised by the perishable commodity results. Its absence is conspicuous given that the reference appears in the bibliography.

10. **The dispersion regression (Section 6.4) clusters at the state level but the outcome is also computed at the state level, creating a mechanical relationship between the unit of observation and the clustering level.** With only 20 states, asymptotic cluster-robust inference may be unreliable. Wild cluster bootstrap p-values should be reported for this specification.

## Suggestions

1. **Present a "control group availability" table** showing, for each event-study quarter relative to treatment, the number of treated mandis and the number of not-yet-treated control mandis available for wheat and soybean. Truncate the event-study plots at the last horizon where a meaningful comparison group exists. This would directly address the GPT-5.2 advisor's concern and clarify how much of the dynamic pattern is identified versus extrapolated.

2. **Implement the Borusyak, Jaravel, and Spiess (2024) imputation estimator** as a third heterogeneity-robust approach. Since the three existing estimators (TWFE, CS-DiD, Sun-Abraham) give qualitatively different answers, a fourth estimator would help adjudicate. BJS is well-suited to settings with limited control groups because it imputes untreated potential outcomes from the pre-treatment period.

3. **Fetch arrival quantity data from the CEDA API** and estimate treatment effects on log arrivals. This was a planned outcome that would substantially strengthen the mechanism analysis. If arrivals are unavailable, the paper should explicitly state this and discuss the implications.

4. **Disclose and discuss the district sampling strategy.** The paper should transparently note that only 6 districts per state were sampled, report what fraction of e-NAM mandis this captures, and ideally re-run the analysis with a different random seed or an expanded sample to assess sensitivity.

5. **Implement HonestDiD sensitivity analysis** for the wheat and soybean CS-DiD estimates. This would allow the reader to assess how much violation of parallel trends the positive results can tolerate---especially important given that two other commodities in the same dataset exhibit substantial pre-trend violations.

6. **Compute between-state price dispersion** (CV of state-level mean prices across states) as a complement to the within-state dispersion measure. If e-NAM promotes inter-state arbitrage, the between-state CV is the more relevant outcome. The null within-state result is unsurprising if all mandis in a state are treated simultaneously.

7. **Report CS-DiD standard errors with state-level clustering**, not just mandi-level clustering. Since the CS-DiD estimates are the paper's headline results and treatment varies at the state level, the current inference may be anti-conservative. If the CS-DiD estimates become insignificant with state-level clustering (as the TWFE estimates do), this substantially weakens the paper's claims.

8. **Add a balance table** comparing observable characteristics (mean pre-treatment prices, price volatility, number of reporting days, number of commodities traded) between early-treated, late-treated, and never-treated mandis. This would help assess the plausibility of the parallel trends assumption and clarify whether early adopters were systematically different.

9. **Consider restricting the analysis window to 2010-2019** to avoid contamination from the COVID-19 pandemic (2020) and the Farm Laws (2020-2021), which the paper acknowledges as concurrent shocks but does not exclude from the sample. While time fixed effects should absorb national shocks, any differential impact of COVID on treated vs. control states would violate parallel trends.

10. **Restructure the paper to lead with the identification challenges rather than burying them.** The current structure presents CS-DiD results as definitive (Section 6.1-6.2), then mentions limitations almost as an afterthought (Section 8.4). A more honest framing would foreground the tension between estimators, the thin control group, and the state-level treatment assignment, and present the positive CS-DiD results as suggestive rather than conclusive evidence.
